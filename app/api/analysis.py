from fastapi import APIRouter
from pydantic import BaseModel
from typing import List

import torch
import torch.nn.functional as F
# KoBERT ì‚¬ìš©ì„ ìœ„í•´ AutoTokenizer ëŒ€ì‹  XLNetTokenizerë¥¼ ì‚¬ìš©í•˜ê³  ê³„ì‹œì§€ë§Œ, 
# KoBERTëŠ” ë³´í†µ KoBERTTokenizer (ë˜ëŠ” AutoTokenizer with specific model name)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 
# XLNetTokenizerë¥¼ KoBERT ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ í˜¸í™˜ì„± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆì§€ë§Œ, 
# ê¸°ì¡´ ì½”ë“œë¥¼ ìœ ì§€í•˜ê³  KoBERT í˜¸í™˜ì„± ë¬¸ì œ ì²˜ë¦¬ ì½”ë“œë¥¼ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
# KoBERT ì‚¬ìš© ì‹œì—ëŠ” 'skt/kobert-base-v1' ëª¨ë¸ê³¼ í•¨ê»˜ AutoTokenizerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
from transformers import AutoModel, XLNetTokenizer 

# KoBERT ëª¨ë¸ ë¡œë“œ
tokenizer = XLNetTokenizer.from_pretrained("skt/kobert-base-v1")
model = AutoModel.from_pretrained("skt/kobert-base-v1")
model.eval()

analysis = APIRouter(prefix="/analysis")

# --------------------------
# 1. ìš”ì²­ ë° ì‘ë‹µ Pydantic ëª¨ë¸ ì •ì˜
# --------------------------

# /get-score ì—”ë“œí¬ì¸íŠ¸ì˜ ì‘ë‹µ ëª¨ë¸
class MonotonyScoreResponse(BaseModel):
    user_id: int
    monotony_score: int

# /make-score ì—”ë“œí¬ì¸íŠ¸ì˜ ìš”ì²­ ëª¨ë¸
class SentenceRequest(BaseModel) :
    sentences: List[str]

# /make-score ì—”ë“œí¬ì¸íŠ¸ì˜ ì‘ë‹µ ëª¨ë¸
class CalculateMonotonyResponse(BaseModel):
    sentence_count: int
    average_similarity: float

# --------------------------
# 2. ì„ë² ë”© í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
# --------------------------

def get_sentence_embedding(sentence, tokenizer, model):
    """ì£¼ì–´ì§„ ë¬¸ì¥ì˜ KoBERT ì„ë² ë”©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    inputs = tokenizer.encode_plus(
        sentence,
        add_special_tokens=True,
        return_tensors="pt"
    )

    # KoBERT ì¼ë¶€ ë²„ì „ í˜¸í™˜ì„ ìœ„í•´ token_type_ids ì œê±° (í•„ìš” ì—†ëŠ” ê²½ìš°ë„ ìˆìŒ)
    if "token_type_ids" in inputs:
        del inputs["token_type_ids"]

    with torch.no_grad():
        outputs = model(**inputs)
        # [CLS] í† í° ì„ë² ë”© (ì²« ë²ˆì§¸ í† í°)ì„ ë¬¸ì¥ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©
        last_hidden = outputs.last_hidden_state 

    return last_hidden[:, 0, :].squeeze(0)

# --------------------------
# 3. API ë¼ìš°í„° (response_model ì¶”ê°€)
# --------------------------



@analysis.post("/make-score", tags=["Monotony Score"], response_model=CalculateMonotonyResponse)
async def calculate_monotony(req: SentenceRequest):
    """
    ì£¼ì–´ì§„ ë¬¸ì¥ë“¤ì˜ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ë‹¨ì¡°ë¡œì›€ ì ìˆ˜ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
    (ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ë‹¨ì¡°ë¡­ë‹¤ê³  ê°„ì£¼)
    """
    sentences = req.sentences

    # ë¬¸ì¥ ì„ë² ë”© ìƒì„±
    embeddings = []
    for text in sentences:
        emb = get_sentence_embedding(text, tokenizer, model)
        embeddings.append(emb)

    if not embeddings:
        return {
            "sentence_count": 0,
            "average_similarity": 0.0
        }

    embedding_matrix = torch.stack(embeddings)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    normalized = F.normalize(embedding_matrix, p=2, dim=1)
    sim_matrix = torch.matmul(normalized, normalized.transpose(0, 1))

    # numpy ë³€í™˜
    sim = sim_matrix.cpu().numpy()

    # ---------------------------
    # ğŸ”¥ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
    # (ìê¸° ìì‹  i==j ëŠ” ì œì™¸)
    # ---------------------------
    n = len(sentences)
    total = 0
    count = 0

    for i in range(n):
        for j in range(n):
            if i != j:
                total += sim[i][j]
                count += 1

    avg_similarity = total / count if count > 0 else 0
    # ì†Œìˆ˜ì  ë‘˜ì§¸ ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼
    avg_similarity = float(f"{avg_similarity:.2f}")

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê°’ (0.00 ~ 1.00)ì„ 0~100 ìŠ¤ì¼€ì¼ë¡œ ë°˜í™˜
    return {
        "sentence_count": n,
        "average_similarity": avg_similarity * 100
    }